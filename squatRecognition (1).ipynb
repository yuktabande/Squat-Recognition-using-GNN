{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content\n",
        "!rm -rf /content/sample_data  # Example: Deleting sample data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pEZcICHncdg",
        "outputId": "9f8cdb93-75bd-4b96-e3df-737912210c23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.0K\n",
            "drwxr-xr-x 1 root root 4.0K Feb 21 14:21 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h /content | sort -rh | head -n 20  # Show largest files\n",
        "!rm -rf /content/your_large_file  # Replace with the actual file path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIwWhyT7nlFp",
        "outputId": "9ced4949-c67a-4559-a129-ae4e78ce2ef5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148K\t/content\n",
            "140K\t/content/.config\n",
            "84K\t/content/.config/logs\n",
            "80K\t/content/.config/logs/2025.02.21\n",
            "8.0K\t/content/.config/configurations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KuGKPYhnyEQ",
        "outputId": "a50931be-b8df-425d-d19d-0d9e404a95ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKvrEusIjzwW",
        "outputId": "152fbd5d-b141-4da0-fbfc-32ffef4b37a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pycocotools opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3_YDh3Xp1CT",
        "outputId": "d34c35da-31f6-4a66-c9bc-faf96e66093f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "0fd1FzWSkrR_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Define the annotation file URL (2017 Train Annotations)\n",
        "annotation_file = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "\n",
        "# Download and extract it in Colab\n",
        "!wget $annotation_file\n",
        "!unzip -q annotations_trainval2017.zip\n",
        "\n",
        "# Load COCO annotations\n",
        "coco = COCO(\"annotations/person_keypoints_train2017.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg8-oAgboyqJ",
        "outputId": "c819adfc-732e-4a94-ec22-8395079376f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
            "--2025-02-26 05:03:17--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.129.177, 52.217.206.9, 3.5.28.199, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.129.177|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  96.1MB/s    in 2.5s    \n",
            "\n",
            "2025-02-26 05:03:20 (96.1 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=9.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load COCO annotations\n",
        "coco = COCO(\"annotations/person_keypoints_train2017.json\")\n",
        "\n",
        "# Define squat-related keypoints\n",
        "KEYPOINTS_IDX = {\"hip_l\": 11, \"hip_r\": 12, \"knee_l\": 13, \"knee_r\": 14, \"ankle_l\": 15, \"ankle_r\": 16}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rza5ewnqozNc",
        "outputId": "ba5e3d97-7f37-40b3-94ee-8b281b27c494"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=8.13s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract squat-related images\n",
        "def extract_squat_data(coco, num_samples=100):\n",
        "    squat_data = []\n",
        "\n",
        "    for img_id in coco.getImgIds():\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
        "        annotations = coco.loadAnns(ann_ids)\n",
        "\n",
        "        for ann in annotations:\n",
        "            if 'keypoints' in ann and len(ann['keypoints']) == 51:  # Ensure all keypoints are present\n",
        "                keypoints = np.array(ann['keypoints']).reshape(-1, 3)  # (x, y, visibility)\n",
        "\n",
        "                # Extract relevant keypoints\n",
        "                hips = keypoints[[KEYPOINTS_IDX[\"hip_l\"], KEYPOINTS_IDX[\"hip_r\"]], :2]\n",
        "                knees = keypoints[[KEYPOINTS_IDX[\"knee_l\"], KEYPOINTS_IDX[\"knee_r\"]], :2]\n",
        "                ankles = keypoints[[KEYPOINTS_IDX[\"ankle_l\"], KEYPOINTS_IDX[\"ankle_r\"]], :2]\n",
        "\n",
        "                # Calculate heuristic: hip below knee → correct squat\n",
        "                squat_correct = (hips[:, 1].mean() > knees[:, 1].mean())  # Compare Y-coordinates\n",
        "\n",
        "                squat_data.append({\n",
        "                    \"image_id\": img_id,\n",
        "                    \"hips\": hips.tolist(),\n",
        "                    \"knees\": knees.tolist(),\n",
        "                    \"ankles\": ankles.tolist(),\n",
        "                    \"label\": int(squat_correct)  # 1 = correct, 0 = incorrect\n",
        "                })\n",
        "\n",
        "        if len(squat_data) >= num_samples:\n",
        "            break  # Stop when we have enough samples\n",
        "\n",
        "    return squat_data"
      ],
      "metadata": {
        "id": "XOTSGm9PpRiV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and save squat data\n",
        "squat_dataset = extract_squat_data(coco, num_samples=500)\n",
        "with open(\"squat_data.json\", \"w\") as f:\n",
        "    json.dump(squat_dataset, f)\n",
        "\n",
        "print(f\"✅ Preprocessed {len(squat_dataset)} squat samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGLd0kzypX1R",
        "outputId": "53c67ee1-99d0-48c6-e895-573e083179f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessed 511 squat samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed squat data\n",
        "with open(\"squat_data.json\", \"r\") as f:\n",
        "    squat_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "dDkrdaAFpa_K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define edges between joints\n",
        "edges = [\n",
        "    (0, 1), (1, 2),  # Left hip → left knee → left ankle\n",
        "    (3, 4), (4, 5),  # Right hip → right knee → right ankle\n",
        "    (0, 3),  # Left hip ↔ Right hip\n",
        "    (2, 5)   # Left ankle ↔ Right ankle\n",
        "]\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()"
      ],
      "metadata": {
        "id": "WGUt2dXLqEky"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into graph format\n",
        "def process_data(squat_dataset):\n",
        "    graph_data = []\n",
        "    for sample in squat_dataset:\n",
        "        # Normalize keypoints (divide by image width/height)\n",
        "        keypoints = np.array(sample[\"hips\"] + sample[\"knees\"] + sample[\"ankles\"])\n",
        "        keypoints = keypoints / np.max(keypoints)  # Normalize\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        x = torch.tensor(keypoints, dtype=torch.float)\n",
        "        y = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
        "\n",
        "        # Create PyTorch Geometric graph\n",
        "        graph = Data(x=x, edge_index=edge_index, y=y)\n",
        "        graph_data.append(graph)\n",
        "\n",
        "    return graph_data"
      ],
      "metadata": {
        "id": "rgL18UlTqH_f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process dataset\n",
        "graph_dataset = process_data(squat_dataset)\n",
        "loader = DataLoader(graph_dataset, batch_size=32, shuffle=True)\n",
        "print(f\"✅ Processed {len(graph_dataset)} squat graphs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3b_DomWqPg7",
        "outputId": "b5cc60b2-fd5d-47e5-8767-1b9d71821247"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed 511 squat graphs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-219f237e9991>:7: RuntimeWarning: invalid value encountered in divide\n",
            "  keypoints = keypoints / np.max(keypoints)  # Normalize\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SquatGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquatGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(2, 16)  # Input: 2D keypoints, Output: 16 features\n",
        "        self.conv2 = GCNConv(16, 8)\n",
        "        self.fc = torch.nn.Linear(8, 2)  # Output: Binary classification (correct/incorrect)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = torch.mean(x, dim=0)  # Global pooling\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "-5rUndIMqSB8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = SquatGNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "hU1SEewmqaxa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Load COCO dataset\n",
        "annotation_file = \"annotations/person_keypoints_train2017.json\"\n",
        "coco = COCO(annotation_file)\n",
        "\n",
        "# Get all person image IDs\n",
        "cat_ids = coco.getCatIds(catNms=['person'])\n",
        "img_ids = coco.getImgIds(catIds=cat_ids)\n",
        "\n",
        "# Print dataset size\n",
        "print(f\"Total images with person keypoints: {len(img_ids)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuKUr-o6sfHF",
        "outputId": "f0d84f8b-8b2d-49b9-bd1e-4b11f6e9db5e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=8.42s)\n",
            "creating index...\n",
            "index created!\n",
            "Total images with person keypoints: 64115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Custom COCO dataset class with padding\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, coco, max_keypoints=51):  # 17 keypoints * 3 (x, y, visibility)\n",
        "        self.coco = coco\n",
        "        self.image_ids = list(self.coco.imgs.keys())\n",
        "        self.max_keypoints = max_keypoints  # Fix the size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        keypoints = []\n",
        "        for ann in annotations:\n",
        "            if 'keypoints' in ann:\n",
        "                keypoints.append(ann['keypoints'])\n",
        "\n",
        "        if len(keypoints) == 0:\n",
        "            keypoints = np.zeros((self.max_keypoints,))  # No keypoints\n",
        "\n",
        "        keypoints = np.array(keypoints).flatten()\n",
        "\n",
        "        # Ensure consistent size (pad or truncate)\n",
        "        if len(keypoints) < self.max_keypoints:\n",
        "            keypoints = np.pad(keypoints, (0, self.max_keypoints - len(keypoints)), mode='constant')\n",
        "        else:\n",
        "            keypoints = keypoints[:self.max_keypoints]\n",
        "\n",
        "        return torch.tensor(keypoints, dtype=torch.float32)\n",
        "\n",
        "# Load dataset\n",
        "dataset = COCODataset(coco)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define a simple keypoint regression model\n",
        "class KeypointNet(nn.Module):\n",
        "    def __init__(self, input_size=51):\n",
        "        super(KeypointNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, input_size)  # Predict keypoints\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = KeypointNet(input_size=51)  # Match dataset size\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for keypoints in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(keypoints)\n",
        "        loss = criterion(outputs, keypoints)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NCD28tvyhhf",
        "outputId": "7027a697-0cb2-4cf9-f80d-53ec45e92b45"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 313.7650623944504\n",
            "Epoch 2, Loss: 21.079754855718555\n",
            "Epoch 3, Loss: 14.327159805929206\n",
            "Epoch 4, Loss: 12.050139219382984\n",
            "Epoch 5, Loss: 13.164273676096443\n",
            "Epoch 6, Loss: 10.295089120457938\n",
            "Epoch 7, Loss: 11.727820601589395\n",
            "Epoch 8, Loss: 8.760311600918186\n",
            "Epoch 9, Loss: 8.424102212501147\n",
            "Epoch 10, Loss: 9.008080368416678\n",
            "Epoch 11, Loss: 12.15439369018606\n",
            "Epoch 12, Loss: 7.108030310201426\n",
            "Epoch 13, Loss: 7.097398653767513\n",
            "Epoch 14, Loss: 6.443824491268369\n",
            "Epoch 15, Loss: 6.796915436487505\n",
            "Epoch 16, Loss: 7.04151498021501\n",
            "Epoch 17, Loss: 6.421876794838215\n",
            "Epoch 18, Loss: 6.065641369365311\n",
            "Epoch 19, Loss: 5.5240538784675355\n",
            "Epoch 20, Loss: 5.737382961272355\n",
            "Epoch 21, Loss: 5.9028812192575915\n",
            "Epoch 22, Loss: 4.541536292434545\n",
            "Epoch 23, Loss: 5.089805304697793\n",
            "Epoch 24, Loss: 5.718228259496441\n",
            "Epoch 25, Loss: 5.092145152456508\n",
            "Epoch 26, Loss: 4.708121847374426\n",
            "Epoch 27, Loss: 4.536306716068422\n",
            "Epoch 28, Loss: 4.020255032877423\n",
            "Epoch 29, Loss: 4.573308608662221\n",
            "Epoch 30, Loss: 4.563462240061697\n",
            "Epoch 31, Loss: 4.082937408116724\n",
            "Epoch 32, Loss: 4.043425350651762\n",
            "Epoch 33, Loss: 4.183814032886491\n",
            "Epoch 34, Loss: 4.260002051676355\n",
            "Epoch 35, Loss: 3.490008600251656\n",
            "Epoch 36, Loss: 3.866629233630664\n",
            "Epoch 37, Loss: 3.6025843804418507\n",
            "Epoch 38, Loss: 3.807948609256312\n",
            "Epoch 39, Loss: 3.9352792780045274\n",
            "Epoch 40, Loss: 4.160094814632498\n",
            "Epoch 41, Loss: 3.1359363210936446\n",
            "Epoch 42, Loss: 3.7304308334895007\n",
            "Epoch 43, Loss: 3.7976532676734567\n",
            "Epoch 44, Loss: 3.031220226202232\n",
            "Epoch 45, Loss: 3.5214282192142714\n",
            "Epoch 46, Loss: 3.209497996539176\n",
            "Epoch 47, Loss: 3.5211529485238446\n",
            "Epoch 48, Loss: 3.1139425925702895\n",
            "Epoch 49, Loss: 3.1097875509450397\n",
            "Epoch 50, Loss: 3.9553355400767685\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load COCO validation annotations\n",
        "coco_val = COCO(\"annotations/person_keypoints_val2017.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTpa-Mex1_UX",
        "outputId": "bf24f61d-c625-45c0-82df-54ffe5cffc99"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.38s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load validation dataset (assuming your dataset class supports COCO annotations)\n",
        "val_dataset = COCODataset(coco_val)  # Ensure your dataset class can handle COCO validation data\n",
        "\n",
        "# Create validation DataLoader\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "vWMok_jA8R2z"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            keypoints, labels = batch[:2]  # Adjust based on output structure\n",
        "            outputs = model(keypoints)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "Zqh1HdmD87TJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function (same as training)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Validate the model\n",
        "val_loss = validate_model(model, val_dataloader, criterion)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjNwgwAs898k",
        "outputId": "d2aebb13-0db7-4e63-9bba-17086ce8edd5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 22812.5562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_error(preds, targets):\n",
        "    return torch.mean(torch.abs(preds - targets))\n",
        "\n",
        "mae = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        keypoints = batch[0]  # Adjust based on your dataset\n",
        "        labels = batch[1]\n",
        "\n",
        "        preds = model(keypoints)\n",
        "        mae += mean_absolute_error(preds, labels).item()\n",
        "\n",
        "mae /= len(val_dataloader)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr1M0Rga9XZJ",
        "outputId": "70c1925c-c0a7-45be-8454-b2823d2f5471"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 74.2512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Define COCO Keypoints Dataset\n",
        "class COCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotation_file, transform=None):\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.img_ids = list(self.coco.imgs.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.img_ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        keypoints = []\n",
        "        for ann in annotations:\n",
        "            if 'keypoints' in ann:\n",
        "                keypoints.append(torch.tensor(ann['keypoints']).view(-1, 3))  # Reshape to (num_keypoints, 3)\n",
        "\n",
        "        if not keypoints:\n",
        "            return None  # Skip if no keypoints\n",
        "\n",
        "        keypoints = torch.stack(keypoints)  # Stack all keypoints\n",
        "\n",
        "        if self.transform:\n",
        "            keypoints = self.transform(keypoints)\n",
        "\n",
        "        return keypoints.float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "# Data transformation (Normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x / 255.0)  # Normalize keypoints\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = COCODataset(\"annotations/person_keypoints_train2017.json\", transform=transform)\n",
        "val_dataset = COCODataset(\"annotations/person_keypoints_val2017.json\", transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
        "\n",
        "# Define the model\n",
        "class KeypointModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KeypointModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(51, 128)  # 17 keypoints * 3 (x, y, visibility)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 51)  # Output same shape as input\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = KeypointModel()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSHgoWSk9ZgI",
        "outputId": "1b108bea-df46-4855-be09-91a8d0e8ce11"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=11.56s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with batch filtering\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = [kp for kp in batch if kp is not None]  # Remove None values\n",
        "        if not batch: continue  # Skip if batch is empty\n",
        "\n",
        "        keypoints = torch.cat(batch).view(-1, 51)  # Flatten\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(keypoints)\n",
        "        loss = criterion(outputs, keypoints)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry5AyjkF-Wzw",
        "outputId": "55e5c474-fd03-4050-c635-4a709d82cc51"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0458\n",
            "Epoch 2, Loss: 0.0492\n",
            "Epoch 3, Loss: 0.0513\n",
            "Epoch 4, Loss: 0.0440\n",
            "Epoch 5, Loss: 0.0469\n",
            "Epoch 6, Loss: 0.0421\n",
            "Epoch 7, Loss: 0.0479\n",
            "Epoch 8, Loss: 0.0419\n",
            "Epoch 9, Loss: 0.0421\n",
            "Epoch 10, Loss: 0.0481\n",
            "Epoch 11, Loss: 0.0475\n",
            "Epoch 12, Loss: 0.0365\n",
            "Epoch 13, Loss: 0.0461\n",
            "Epoch 14, Loss: 0.0418\n",
            "Epoch 15, Loss: 0.0402\n",
            "Epoch 16, Loss: 0.0449\n",
            "Epoch 17, Loss: 0.0435\n",
            "Epoch 18, Loss: 0.0381\n",
            "Epoch 19, Loss: 0.0346\n",
            "Epoch 20, Loss: 0.0405\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, dataloader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient updates during validation\n",
        "        for batch in dataloader:\n",
        "            batch = [kp for kp in batch if kp is not None]  # Remove None values\n",
        "            if not batch: continue  # Skip empty batches\n",
        "\n",
        "            keypoints = torch.cat(batch).view(-1, 51)  # Flatten\n",
        "            outputs = model(keypoints)\n",
        "            loss = criterion(outputs, keypoints)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "val_loss = validate_model(model, val_dataloader, criterion)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD_WSmCsHX_u",
        "outputId": "f6a03d42-f2ac-47a1-eb75-4d1e90f38969"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_error(preds, targets):\n",
        "    return torch.mean(torch.abs(preds - targets))\n",
        "\n",
        "mae = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        batch = [kp for kp in batch if kp is not None]  # Remove None values\n",
        "        if not batch: continue  # Skip empty batches\n",
        "\n",
        "        keypoints = torch.cat(batch).view(-1, 51)  # Flatten\n",
        "        preds = model(keypoints)\n",
        "        mae += mean_absolute_error(preds, keypoints).item()\n",
        "\n",
        "mae /= len(val_dataloader)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoHxs7WWHpTo",
        "outputId": "9392ad8d-1967-4c8a-924e-7e1cdba1e1e8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_keypoints(image, keypoints, ax):\n",
        "    \"\"\"Plot keypoints on an image.\"\"\"\n",
        "    if image.dim() == 2:\n",
        "        image = image.unsqueeze(0)\n",
        "    elif image.dim() == 4:\n",
        "        image = image[0]\n",
        "\n",
        "    if image.shape[0] == 1:\n",
        "        image = image.repeat(3, 1, 1)\n",
        "\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    image = torch.clamp(image, 0, 1)  # 🔥 Fix: Clip pixel values to [0,1]\n",
        "\n",
        "    ax.imshow(image.cpu().numpy())\n",
        "\n",
        "    if keypoints.numel() % 2 != 0:\n",
        "        print(f\"Skipping batch due to invalid keypoints shape: {keypoints.shape}\")\n",
        "        return\n",
        "\n",
        "    keypoints = keypoints.view(-1, 2).cpu().numpy()\n",
        "    for x, y in keypoints:\n",
        "        ax.scatter(x, y, c='red', s=10)\n",
        "\n",
        "def test_model(model, dataloader, num_samples=5):\n",
        "    \"\"\"Test model and visualize predictions.\"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    sample_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            if batch is None or len(batch) < 2:\n",
        "                print(\"Skipping empty batch\")\n",
        "                continue\n",
        "\n",
        "            images, keypoints = batch[:2]\n",
        "\n",
        "            if images is None or keypoints is None:\n",
        "                print(\"Skipping batch with missing images or keypoints\")\n",
        "                continue\n",
        "\n",
        "            if keypoints.numel() % 2 != 0:\n",
        "                print(f\"Skipping batch due to invalid keypoints shape: {keypoints.shape}\")\n",
        "                continue\n",
        "\n",
        "            keypoints = keypoints.view(keypoints.shape[0], -1)\n",
        "\n",
        "            preds = model(keypoints)\n",
        "\n",
        "            if images[0] is None:\n",
        "                print(\"Skipping batch with missing image\")\n",
        "                continue\n",
        "\n",
        "            plot_keypoints(images[0], preds, axes[sample_count])\n",
        "            axes[sample_count].axis(\"off\")\n",
        "\n",
        "            sample_count += 1\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run testing\n",
        "test_model(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3vpyy9JtHsMY",
        "outputId": "66b6aa22-2848-4368-a111-3526b6034151"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([5, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([3, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([11, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([1, 17, 3])\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch with missing images or keypoints\n",
            "Skipping batch due to invalid keypoints shape: torch.Size([9, 17, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPLcF4ajOmgN",
        "outputId": "b748f2c5-f85c-44bb-81a8-6fbe1d63286a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "example_input = torch.randn(1, 51)\n",
        "\n",
        "# Convert model to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    example_input,\n",
        "    \"model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
        ")\n",
        "\n",
        "print(\"Model has been converted to ONNX and saved as 'model.onnx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6_tYlswH7up",
        "outputId": "a4bf18ff-2196-48b9-d5ec-a3e54c6e15ee"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been converted to ONNX and saved as 'model.onnx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(\"model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"ONNX model is valid!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8c3XB67OyY0",
        "outputId": "aa80b6b1-2ffd-4136-db17-210e08114855"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model is valid!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pu4mwhfpOzQD",
        "outputId": "74a2a1da-f7e9-4aa7-f13e-55e65c58628d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8d364028-816d-43ef-b98f-0608d83067cc\", \"model.onnx\", 73618)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aI28zWAO8qh",
        "outputId": "7268be78-0d39-4bfb-9ded-923727474ef1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load ONNX model\n",
        "ort_session = ort.InferenceSession(\"model.onnx\")\n",
        "\n",
        "# Prepare input (example input size: [1, 51])\n",
        "input_data = torch.randn(1, 51).numpy().astype(np.float32)\n",
        "\n",
        "# Run inference\n",
        "outputs = ort_session.run(None, {\"input\": input_data})\n",
        "print(\"ONNX model output:\", outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utasURMhPBN_",
        "outputId": "204f8651-8222-4aa2-d48f-5f3e9645f7d8"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model output: [array([[ 0.6861745 ,  0.79668844, -0.647079  ,  0.3946628 ,  1.0559177 ,\n",
            "        -0.39261967,  0.60481155, -0.27558264, -0.7348731 ,  1.7240148 ,\n",
            "        -0.2847821 ,  0.00555047,  0.7176297 ,  0.03418076, -0.19402237,\n",
            "         0.28676218, -0.31372026, -0.10253859,  0.43253976, -0.09764801,\n",
            "        -0.23150596,  1.0297122 ,  1.1737008 ,  0.11942338, -1.5891423 ,\n",
            "         0.53379554, -0.29377756, -0.14529687, -0.18289609,  0.20062056,\n",
            "         0.39441618, -0.5240064 , -0.19453071, -0.0990414 ,  0.5287106 ,\n",
            "         0.22534171, -0.19081399,  1.1077014 ,  0.12594269, -0.53273   ,\n",
            "        -0.0642308 , -0.34860379,  0.25146097,  1.2909805 , -0.25885892,\n",
            "         0.31700924,  0.6018197 , -0.21251695,  1.188483  ,  1.2329522 ,\n",
            "        -0.46131107]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbd--x0-PF8p",
        "outputId": "048b8e10-8a9b-4359-848b-9f9f976b5105"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.8 starlette-0.45.3 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHHttdSLQV2I",
        "outputId": "c4812376-25ca-4d0a-de4d-f3608cd9be6f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn python-multipart pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X__QXMXgQXjj",
        "outputId": "77733a5b-0dd6-4e9e-9971-b17f9fde6b01"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"YOUR_NGROK_AUTHTOKEN\"  # Don't upload this line with the real token!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n_7xrG6Qvq9",
        "outputId": "c7fd65da-5d64-49fb-def0-bc7224e9073f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Start Ngrok\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run FastAPI in the background\n",
        "def run():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATE5EK_5PKKT",
        "outputId": "38451337-0c95-4dfa-e06f-b88605686214"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://81a1-104-196-61-85.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDnoJYjgPMOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}